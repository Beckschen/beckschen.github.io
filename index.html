<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'UA-7580334-2');
  </script>

  <title>Jieneng Chen</title>
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css">
  
  <meta name="author" content="Jie-Neng Chen">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="stylesheet" type="text/css" href="css/style.css">
  <link rel="icon" type="image/png" href="myimages/icon_jhu.jpeg">
  <style>
    pre {
        background-color: #f5f5f5;
        padding: 0px;
        border: 1px solid #ccc;
        border-radius: 5px;
        overflow: auto;
    }
    code {
        font-family: 'Courier New', Courier, monospace;
        font-size: 8px;
        color: #333;
    }
</style>
</head>


<!-- <body onload="startPetCursor();"> -->
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:70%;vertical-align:middle">
              <p style="text-align:center">
                <name>Jieneng Chen</name>
              </p>
              <p>Jieneng is a fourth-year Ph.D. candidate in <a href="https://www.cs.jhu.edu/">Computer Science</a> at <a href="https://www.jhu.edu/">Johns Hopkins University</a>, advised by Distinguished Professor <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>. He devotes time to the research projects with the <a href="https://ccvl.jhu.edu">CCVL</a> (Computational Cognition, Vision, and Learning), the <a href="https://www.clsp.jhu.edu/">CLSP</a> (Center for Language and Speech Processing) and the <a href="https://www.hopkinsmedicine.org/">JHMI</a> (Johns Hopkins Medicine). 
              </p>

              <p>
                His research focuses on transforming highly complex multi-modal signals into semantic representations that are more easily understood and processed by machines, a critical step towards human-level AI. To achieve this, he innovates architectural designs for Transformers, vision-language models, and large multi-modal models. During his journey, he has had the fortune to co-develop a few highly-cited algorithms in computer vision and medical applications. 
              </p>

              
              <p style="text-align:center">
                </br>
                <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
                <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
                
                <a href="https://scholar.google.com/citations?hl=en&user=yLYj88sAAAAJ"><i class="ai ai-google-scholar ai-3x" style='font-size:30px'></i>&nbsp &nbsp
                <a href="mailto:jienengchen01@gmail.com"><i class="fa fa-envelope" style='font-size:29px'></i>&nbsp &nbsp
                <a href="https://github.com/Beckschen"><i class="fa fa-github" style='font-size:30px'></i>&nbsp &nbsp
                <a href="https://www.linkedin.com/in/jieneng-chen-53254011a/"><i class="fa fa-linkedin" style='font-size:30px'></i>&nbsp &nbsp
              
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <a href="myimages/photo.jpg"><img style="width:80%;max-width:80%;padding:0;border:1px solid #f2f3f3" alt="profile photo" src="myimages/photo.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
  
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:10px;width:100%;vertical-align:middle">
            <heading>News</heading>
          </td>
        </tr>
      </tbody></table>
    <ul> 
      <li>[July 2024] Check out <a href="https://arxiv.org/abs/2406.20092">LLaVolta</a>, an efficient large multi-modal model.
      </li> 
    </ul>
      <ul> 
        <li>[June 2024] Our multi-modal foundation model, the <a href="https://arxiv.org/pdf/2404.02132.pdf">ViTamin</a> model, has been incorporated into two leading machine learning codebases: <strong><a href="https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/vitamin.py">timm</a></strong> <img src="https://img.shields.io/github/stars/huggingface/pytorch-image-models?style=social&label=Star&maxAge=2592000" alt="GitHub Stars Badge" style="margin-top: -3px; width:auto; height:18px;"> and <strong><a href="https://github.com/mlfoundations/open_clip/blob/main/src/open_clip/model_configs/ViTamin-XL-384.json">open_clip</a></strong> <img src="https://img.shields.io/github/stars/mlfoundations/open_clip?style=social&label=Star&maxAge=2592000" alt="GitHub Stars Badge" style="margin-top: -3px; width:auto; height:18px;">. The model is downloaded 1K+ times per month on HuggingFace.
        </li> 
      </ul>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <heading>Selected Publications</heading>
            </td>
          </tr>
        </tbody>
      </table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>        



          <p><font size="4" style="position: relative; padding-left: 20px;"><a href="https://scholar.google.com/citations?hl=en&user=yLYj88sAAAAJ">Full list on Google Scholar Profile</a>. </font> His publications have over 8,000 citations (as of Jul. 2024) with an increase of over 5,000 per year. </p> 



          <tr>
            <td width="75%" valign="middle" style="position: relative; padding-left: 40px;">
              <div style="position: absolute; top: 5px; left: 20px; height: 70%; border-left: 3px solid #000000;"></div>
                  <p>
                  <a href="https://arxiv.org/pdf/2404.02132.pdf">
                      <papertitle>Designing Scalable Vision Models in the Vision-Language Era</papertitle>
                  </a>
                  <br>
                  <strong>Jieneng Chen</strong>, Qihang Yu, Xiaohui Shen, Alan Yuille, Liang-Chieh Chen
                  <br>
                  <p></p>
                  In Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2024
                  <br>
                  <a href="https://arxiv.org/pdf/2404.02132.pdf">Paper</a> |
                      <a href="https://github.com/Beckschen/ViTamin">Code</a> |
                        <a href="https://huggingface.co/jienengchen/ViTamin-XL-384px">ðŸ¤— HuggingFace</a>
                </p>
                 <br>
             </td>
         </tr>

          <tr>
            <td width="75%" valign="middle" style="position: relative; padding-left: 40px;">
              <div style="position: absolute; top: 5px; left: 20px; height: 70%; border-left: 3px solid #000000;"></div>
                  <p>
                  <a href="https://arxiv.org/abs/2406.20092">
                      <papertitle>LLaVolta: Efficient Multi-modal Models via Stage-wise Visual Context Compression</papertitle>
                  </a>
                  <br>
                  <strong>Jieneng Chen</strong>, Luoxin Ye, Ju He, Zhao-Yang Wang, Daniel Khashabi, Alan Yuille
                  <br>
                  <p></p>
                  Technical Report, 2024
                  <br>
                  <a href="https://arxiv.org/pdf/2406.20092">Paper</a> |
                      <a href="https://github.com/Beckschen/LLaVolta">Code</a> |
                        <a href="https://beckschen.github.io/llavolta.html">Project</a>
                </p>
                 <br>
             </td>
         </tr>


          <tr>
            <td width="75%" valign="middle" style="position: relative; padding-left: 40px;">
              <div style="position: absolute; top: 5px; left: 20px; height: 75%; border-left: 3px solid #000000;"></div>
                  <p>
                  <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_CancerUniT_Towards_a_Single_Unified_Model_for_Effective_Detection_Segmentation_ICCV_2023_paper.pdf">
                      <papertitle>Towards a Single Unified Model for Effective Detection, Segmentation, and Diagnosis of Eight Major Cancers</papertitle>
                  </a>
                  <br>
                  <strong>Jieneng Chen</strong>, Yingda Xia, Jiawen Yao, Ke Yan, Jianpeng Zhang, Le Lu, .., Jingren Zhou, Alan Yuille, Zaiyi Liu, Ling Zhang
                  <br>
                  <p></p>
                  In International Conference on Computer Vision (<strong>ICCV</strong>), 2023
                  <br>
                      <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_CancerUniT_Towards_a_Single_Unified_Model_for_Effective_Detection_Segmentation_ICCV_2023_paper.pdf">Paper</a> 
                </p>
                 <br>
             </td>
         </tr>

        <tr>
            <td width="75%" valign="middle" style="position: relative; padding-left: 40px;">
              <div style="position: absolute; top: 5px; left: 20px; height: 70%; border-left: 3px solid #000000;"></div>
                  <p>
                  <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/He_Compositor_Bottom-Up_Clustering_and_Compositing_for_Robust_Part_and_Object_CVPR_2023_paper.pdf">
                      <papertitle>Compositor: Bottom-up Clustering and Compositing for Robust Part and Object Segmentation</papertitle>
                  </a>
                  <br>
                  Ju He *, <strong>Jieneng Chen *</strong>, Mingxian Lin, Qihang Yu, Alan Yuille
                  <br>
                  <p></p>
                  In Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2023
                  <br>
                      <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/He_Compositor_Bottom-Up_Clustering_and_Compositing_for_Robust_Part_and_Object_CVPR_2023_paper.pdf">Paper</a> |
                      * Equal contributed
                </p>
                 <br>
             </td>
         </tr>

        <tr>
            <td width="75%" valign="middle" style="position: relative; padding-left: 40px;">
              <div style="position: absolute; top: 5px; left: 20px; height: 70%; border-left: 3px solid #000000;"></div>
                  <p>
                  <a href="https://arxiv.org/abs/2111.09833?context=cs">
                      <papertitle>TransMix: Attend to Mix for Vision Transformers</papertitle>
                  </a>
                  <br>
                  <strong>Jieneng Chen</strong>, Shuyang Sun, Ju He, Philip Torr, Alan Yuille, Song Bai
                  <br>
                  <p></p>
                  In Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2022
                  <br>
                      <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_TransMix_Attend_To_Mix_for_Vision_Transformers_CVPR_2022_paper.pdf">Paper</a> | 
                      <a href="https://github.com/Beckschen/TransMix">Code</a> 
                      <!-- <img src=https://img.shields.io/github/stars/Beckschen/TransMix?style=social&label=Star&maxAge=2592000 alt> -->
    
                </p>
                 <br>
             </td>
         </tr>


         <tr>
          <td width="75%" valign="middle" style="position: relative; padding-left: 40px;">
            <div style="position: absolute; top: 5px; left: 20px; height: 70%; border-left: 3px solid #000000;"></div>
                <p>
                <a href="https://arxiv.org/abs/2211.15846">
                    <papertitle>LUMix: Improving Mixup by Better Modelling Label Uncertainty</papertitle>
                </a>
                <br>
                Shuyang Sun *, <strong>Jieneng Chen *</strong>, Ruifei He, Alan Yuille, Philip Torr, Song Bai
                <br>
                <p></p>
                In International Conference on Acoustics, Speech, and Signal Processing (<strong>ICASSP</strong>), arXiv 2022
                <br>
                    <a href="https://arxiv.org/pdf/2211.15846.pdf">Paper</a> |
                    * Equal contributed
              </p>
               <br>
           </td>
       </tr>

         <tr>
          <td width="75%" valign="middle" style="position: relative; padding-left: 40px;">
            <div style="position: absolute; top: 5px; left: 20px; height: 65%; border-left: 3px solid #000000;"></div>
                 <p>
                     <a href="https://arxiv.org/pdf/2102.04306.pdf">
                         <papertitle>3D TransUNet: Advancing Medical Image Segmentation Through Vision Transformers</papertitle>
                     </a>
                     <br>
                     <strong>Jieneng Chen</strong>, Jieru Mei, Xianhang Li, Yongyi Lu, Qihang Yu, Qingyue Wei, Xiangde Luo, Yutong Xie, Ehsan Adeli, Yan Wang, Matthew Lungren, Lei Xing, Le Lu, Alan Yuille, Yuyin Zhou
                     <br>
                     <p></p>
                     Medical Image Analysis, 2024
                     <br>
                      <a href="https://arxiv.org/pdf/2310.07781">Paper</a> |
                      <a href="https://github.com/Beckschen/3D-TransUNet">Code</a> |
                      <img src="https://img.shields.io/github/stars/Beckschen/3D-TransUNet?style=social&label=Star&maxAge=2592000" alt="GitHub Stars Badge" style="margin-top: -3px; width:auto; height:22px;"> 
                </p>
                <br>
             </td>
         </tr>


         <tr>
          <td width="75%" valign="middle" style="position: relative; padding-left: 40px;">
            <div style="position: absolute; top: 5px; left: 20px; height: 65%; border-left: 3px solid #000000;"></div>
                 <p>
                     <a href="https://arxiv.org/pdf/2102.04306.pdf">
                         <papertitle>TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation</papertitle>
                     </a>
                     <br>
                     <strong>Jieneng Chen</strong>, Yongyi Lu, Qihang Yu, Xiangde Luo, Ehsan Adeli, Yan Wang, Le Lu, Alan Yuille, Yuyin Zhou
                     <br>
                     <p></p>
                     International Conference on Marchine Learning (<strong>ICML</strong>) workshop, arXiv 2021
                     <br>
                      <a href="https://arxiv.org/pdf/2102.04306.pdf">Paper</a> |
                      <a href="https://github.com/Beckschen/TransUNet">Code</a> |
                      <img src="https://img.shields.io/github/stars/Beckschen/TransUNet?style=social&label=Star&maxAge=2592000" alt="GitHub Stars Badge" style="margin-top: -3px; width:auto; height:22px;"> 
                </p>
                <br>
             </td>
         </tr>


         <tr>
          <td width="75%" valign="middle" style="position: relative; padding-left: 40px;">
            <div style="position: absolute; top: 5px; left: 20px; height: 65%; border-left: 3px solid #000000;"></div>
                 <a href="https://arxiv.org/abs/2103.07976">
                     <papertitle> TransFG: A Transformer Architecture for Fine-grained Recognition </papertitle>
                 </a>
                 <br>
                 Ju He,
                 <strong>Jieneng Chen</strong>, Shuai Liu, Adam Kortylewski, Cheng Yang, Yutong Bai, Changhu Wang, Alan Yuille
                 <br>
                 <p></p>
                 In AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), 2022
                 <br>
                 <a href="https://arxiv.org/pdf/2103.07976.pdf">Paper</a> |
                 <a href="https://github.com/TACJu/TransFG">Code</a>
                </p>
                 <br>
             </td>
         </tr>


         <tr>
          <td width="75%" valign="middle" style="position: relative; padding-left: 40px;">
            <div style="position: absolute; top: 5px; left: 20px; height: 75%; border-left: 3px solid #000000;"></div>
                 <a href="https://arxiv.org/abs/2103.05170">
                     <papertitle> Sequential Learning on Liver Tumor Boundary Semantics and Prognostic Biomarker Mining </papertitle>
                 </a>
                 <br>
                 <strong>Jieneng Chen</strong>, Ke Yan, Yu-Dong Zhang, Youbao Tang, Xun Xu, Shuwen Sun, Qiuping Liu, Lingyun Huang, Jing Xiao, Alan L Yuille, Ya Zhang, Le Lu
                 <br>
                 <p></p>
                In International Conference on Medical Image Computing and Computer-Assisted Intervention (<strong>MICCAI</strong>), 2021
                 <br>
                 <a href="https://arxiv.org/pdf/2103.05170.pdf">Paper</a> | Early Accept | Travel Award
                </p>
                 <br>
             </td>
         </tr>

         <tr>
          <td width="75%" valign="middle" style="position: relative; padding-left: 40px;">
            <div style="position: absolute; top: 5px; left: 20px; height: 75%; border-left: 3px solid #000000;"></div>
                 <a href="https://link.springer.com/chapter/10.1007/978-3-030-59725-2_53?ref=https://giter.site">
                     <papertitle> CPM-Net: A 3D center-points matching network for pulmonary nodule detection in CT scans </papertitle>
                 </a>
                 <br>
                 Tao Song *, <strong>Jieneng Chen *</strong>, Xiangde Luo, Yechong Huang, Xinglong Liu, Ning Huang, Yinan Chen, Zhaoxiang Ye, Huaqiang Sheng, Shaoting Zhang, Guotai Wang
                 <br>
                 <p></p>
                In International Conference on Medical Image Computing and Computer-Assisted Intervention (<strong>MICCAI</strong>), 2020
                 <br>
                 <a href="https://link.springer.com/chapter/10.1007/978-3-030-59725-2_53?ref=https://giter.site">Paper</a> | Early Accept
                </p>
                 <br>
             </td>
         </tr>

         <tr><td width="10%" valign="middle" style="position: relative; padding-left: 20px;">
          <strong>Serving</strong>: invited reviewer and program committee for major conference and journals, such as TPAMI, CVPR, ICCV, ECCV, NeurIPS, ICML, TMI and MICCAI
        </td>
        <br>
        <br>
         </tr>





 <script xml:space="preserve" language="JavaScript">
 hideallbibs();
 </script>

 <!-- Google Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-131560165-1', 'auto');
  ga('send', 'pageview');
</script>
<!-- End Google Analytics -->

</body>

</html>
