<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'UA-7580334-2');
  </script>

  <title>Jieneng Chen</title>
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css">
  
  <meta name="author" content="Jie-Neng Chen">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="stylesheet" type="text/css" href="css/style.css">
  <link rel="icon" type="image/png" href="myimages/icon_jhu.jpeg">
  <style>
    pre {
        background-color: #f5f5f5;
        padding: 0px;
        border: 1px solid #ccc;
        border-radius: 5px;
        overflow: auto;
    }
    code {
        font-family: 'Courier New', Courier, monospace;
        font-size: 8px;
        color: #333;
    }
</style>
</head>


<!-- <body onload="startPetCursor();"> -->
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:70%;vertical-align:middle">
              <p style="text-align:center">
                <name>Jieneng Chen</name>
              </p>
              <p>Jieneng is a fourth-year Ph.D. candidate in <a href="https://www.cs.jhu.edu/">Computer Science</a> at <a href="https://www.jhu.edu/">Johns Hopkins University</a>, advised by Distinguished Professor <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>. 
              </p>

              <p>
                His research primarily focuses on designing intelligent encoders that convert highly complex visual signals into semantic and structured representations, which are more easily decoded for intelligent tasks such as perception, generation, and action. This transformation is crucial for making complex raw signals more comprehensible by machines, marking a significant step toward achieving human-level artificial intelligence.
                </p>  

              <p>
                During his PhD journey, he co-develops a few highly-cited algorithms in computer vision and medical applications, which are among <a href="https://gist.github.com/sergicastellasape/185f72fece3bb489f79366908594504d">the top 15 cited 2021 paper in all AI fields</a> (TransUNet), <a href="https://www.paperdigest.org/2023/04/most-influential-aaai-papers-2023-04/">top 3 most influential AAAI 2023 papers</a> (TransFG), and <a href="https://scholar.google.com/citations?hl=en&vq=en&view_op=list_hcore&venue=cwIh2C-xo8kJ.2024">top 3 most cited ECCV papers in five years in Google Metrics</a> (SwinUNet).
              </p>

              
              <p style="text-align:center">
                </br>
                <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
                <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
                
                <a href="https://scholar.google.com/citations?hl=en&user=yLYj88sAAAAJ"><i class="ai ai-google-scholar ai-3x" style='font-size:30px'></i>&nbsp &nbsp
                <a href="mailto:jienengchen01@gmail.com"><i class="fa fa-envelope" style='font-size:29px'></i>&nbsp &nbsp
                <a href="https://github.com/Beckschen"><i class="fa fa-github" style='font-size:30px'></i>&nbsp &nbsp
                <a href="https://www.linkedin.com/in/jieneng-chen-53254011a/"><i class="fa fa-linkedin" style='font-size:30px'></i>&nbsp &nbsp
              
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <a href="myimages/photo.jpg"><img style="width:80%;max-width:80%;padding:0;border:1px solid #f2f3f3" alt="profile photo" src="myimages/photo.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
  
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:10px;width:100%;vertical-align:middle">
            <heading>News</heading>
          </td>
        </tr>
      </tbody></table>
    <ul> 
      <li>[July 2024] Check out <a href="https://arxiv.org/abs/2406.20092">LLaVolta</a>, an efficient large multi-modal model.
      </li> 
    </ul>
      <ul> 
        <li>[June 2024] Our multi-modal foundation model, the <a href="https://arxiv.org/pdf/2404.02132.pdf">ViTamin</a> model, has been incorporated into two leading machine learning codebases: <strong><a href="https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/vitamin.py">timm</a></strong> <img src="https://img.shields.io/github/stars/huggingface/pytorch-image-models?style=social&label=Star&maxAge=2592000" alt="GitHub Stars Badge" style="margin-top: -3px; width:auto; height:18px;"> and <strong><a href="https://github.com/mlfoundations/open_clip/blob/main/src/open_clip/model_configs/ViTamin-XL-384.json">open_clip</a></strong> <img src="https://img.shields.io/github/stars/mlfoundations/open_clip?style=social&label=Star&maxAge=2592000" alt="GitHub Stars Badge" style="margin-top: -3px; width:auto; height:18px;">. The model is downloaded 1K+ times per month on HuggingFace.
        </li> 
      </ul>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <heading>Selected Publications</heading>
            </td>
          </tr>
        </tbody>
      </table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>        



          <p><font size="4" style="position: relative; padding-left: 20px;"><a href="https://scholar.google.com/citations?hl=en&user=yLYj88sAAAAJ">Full list on Google Scholar Profile</a>. </font> His publications have over 8,000 citations (as of Jul. 2024) with an increase of over 5,000 per year. </p> 



          <tr>
            <td width="75%" valign="middle" style="position: relative; padding-left: 40px;" bgcolor="#ffffd0">
              <div style="position: absolute; top: 5px; left: 20px; height: 85%; border-left: 3px solid #000000;"></div>
                  <p>
                  <a href="https://arxiv.org/pdf/2404.02132.pdf">
                      <papertitle>Designing Scalable Vision Models in the Vision-Language Era</papertitle>
                  </a>
                  <br>
                  <strong>Jieneng Chen</strong>, Qihang Yu, Xiaohui Shen, Alan Yuille, Liang-Chieh Chen
                  <br>
                  <p></p>
                  In Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2024
                  <br>
                  <a href="https://arxiv.org/pdf/2404.02132.pdf">Paper</a> |
                      <a href="https://github.com/Beckschen/ViTamin">Code</a> |
                        <a href="https://huggingface.co/jienengchen/ViTamin-XL-384px">ðŸ¤— HuggingFace</a> | <a href="https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/vitamin.py">Timm</a> | <a href="https://github.com/mlfoundations/open_clip/blob/main/src/open_clip/model_configs/ViTamin-XL-384.json">OpenCLIP</a>
                </p>
             </td>
         </tr>

          <tr>
            <td width="75%" valign="middle" style="position: relative; padding-left: 40px;">
              <div style="position: absolute; top: 18px; left: 20px; height: 70%; border-left: 3px solid #000000;"></div>
                   <br>
                  <p>
                  <a href="https://arxiv.org/abs/2406.20092">
                      <papertitle>LLaVolta: Efficient Multi-modal Models via Stage-wise Visual Context Compression</papertitle>
                  </a>
                  <br>
                  <strong>Jieneng Chen</strong>, Luoxin Ye, Ju He, Zhao-Yang Wang, Daniel Khashabi, Alan Yuille
                  <br>
                  <p></p>
                  Technical Report, 2024
                  <br>
                  <a href="https://arxiv.org/pdf/2406.20092">Paper</a> |
                      <a href="https://github.com/Beckschen/LLaVolta">Code</a> |
                        <a href="https://beckschen.github.io/llavolta.html">Project</a>
                </p>
                 <br>
             </td>
         </tr>


          <tr>
            <td width="75%" valign="middle" style="position: relative; padding-left: 40px;">
              <div style="position: absolute; top: 5px; left: 20px; height: 75%; border-left: 3px solid #000000;"></div>
                  <p>
                  <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_CancerUniT_Towards_a_Single_Unified_Model_for_Effective_Detection_Segmentation_ICCV_2023_paper.pdf">
                      <papertitle>Towards a Single Unified Model for Effective Detection, Segmentation, and Diagnosis of Eight Major Cancers</papertitle>
                  </a>
                  <br>
                  <strong>Jieneng Chen</strong>, Yingda Xia, Jiawen Yao, Ke Yan, Jianpeng Zhang, Le Lu, .., Jingren Zhou, Alan Yuille, Zaiyi Liu, Ling Zhang
                  <br>
                  <p></p>
                  In International Conference on Computer Vision (<strong>ICCV</strong>), 2023
                  <br>
                      <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_CancerUniT_Towards_a_Single_Unified_Model_for_Effective_Detection_Segmentation_ICCV_2023_paper.pdf">Paper</a> 
                </p>
                 <br>
             </td>
         </tr>

        <tr>
            <td width="75%" valign="middle" style="position: relative; padding-left: 40px;">
              <div style="position: absolute; top: 5px; left: 20px; height: 70%; border-left: 3px solid #000000;"></div>
                  <p>
                  <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/He_Compositor_Bottom-Up_Clustering_and_Compositing_for_Robust_Part_and_Object_CVPR_2023_paper.pdf">
                      <papertitle>Compositor: Bottom-up Clustering and Compositing for Robust Part and Object Segmentation</papertitle>
                  </a>
                  <br>
                  Ju He *, <strong>Jieneng Chen *</strong>, Mingxian Lin, Qihang Yu, Alan Yuille
                  <br>
                  <p></p>
                  In Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2023
                  <br>
                      <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/He_Compositor_Bottom-Up_Clustering_and_Compositing_for_Robust_Part_and_Object_CVPR_2023_paper.pdf">Paper</a> |
                      * Equal contributed
                </p>
                 <br>
             </td>
         </tr>

        <tr>
            <td width="75%" valign="middle" style="position: relative; padding-left: 40px;">
              <div style="position: absolute; top: 5px; left: 20px; height: 70%; border-left: 3px solid #000000;"></div>
                  <p>
                  <a href="https://arxiv.org/abs/2111.09833?context=cs">
                      <papertitle>TransMix: Attend to Mix for Vision Transformers</papertitle>
                  </a>
                  <br>
                  <strong>Jieneng Chen</strong>, Shuyang Sun, Ju He, Philip Torr, Alan Yuille, Song Bai
                  <br>
                  <p></p>
                  In Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2022
                  <br>
                      <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_TransMix_Attend_To_Mix_for_Vision_Transformers_CVPR_2022_paper.pdf">Paper</a> | 
                      <a href="https://github.com/Beckschen/TransMix">Code</a> 
                </p>
                 <br>
             </td>
         </tr>

<!--          <tr>
          <td width="75%" valign="middle" style="position: relative; padding-left: 40px;">
            <br>
            <div style="position: absolute; top: 20px; left: 20px; height: 72%; border-left: 3px solid #000000;"><br></div>
                 <a href="https://arxiv.org/abs/2103.07976">
                     <papertitle> TransFG: A Transformer Architecture for Fine-grained Recognition </papertitle>
                 </a>
                 <br>
                 Ju He,
                 <strong>Jieneng Chen</strong>, Shuai Liu, Adam Kortylewski, Cheng Yang, Yutong Bai, Changhu Wang, Alan Yuille
                 <br>
                 <p></p>
                 In AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), 2022
                 <br>
                 <a href="https://arxiv.org/pdf/2103.07976.pdf">Paper</a> |
                 <a href="https://github.com/TACJu/TransFG">Code</a>
                    <br>
                    <br>
              <p style="color: red;", href="https://www.paperdigest.org/2023/04/most-influential-cvpr-papers-2023-04/">Top 3 Most Influential AAAI 2023 Papers</p>
            <br>
             </td>
         </tr> -->



         <tr>
          <td width="75%" valign="middle" style="position: relative; padding-left: 40px;" bgcolor="#ffffd0">
            <div style="position: absolute; top: 5px; left: 20px; height: 92%; border-left: 3px solid #000000;"></div>
                 <p>
                     <a href="https://arxiv.org/pdf/2102.04306.pdf">
                         <papertitle>TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation</papertitle>
                     </a>
                     <br>
                     <strong>Jieneng Chen</strong>, Yongyi Lu, Qihang Yu, Xiangde Luo, Ehsan Adeli, Yan Wang, Le Lu, Alan Yuille, Yuyin Zhou
                     <br>
                     <p></p>
                     International Conference on Marchine Learning (<strong>ICML</strong>) workshop, arXiv 2021
                     <p></p>
                     <a href="https://arxiv.org/pdf/2310.07781">3D-TransUNet</a> is published in Medical Image Analysis, 2024 (IF>10)
                     <br>
                      <a href="https://arxiv.org/pdf/2102.04306.pdf">Paper</a> |
                      <a href="https://github.com/Beckschen/TransUNet">Code</a> |
                      <img src="https://img.shields.io/github/stars/Beckschen/TransUNet?style=social&label=Star&maxAge=2592000" alt="GitHub Stars Badge" style="margin-top: -3px; width:auto; height:22px;"> 
                      <br>
                      <br>
              <i style="color: red;">Top 15 Cited 2021 Paper in All AI Fields (Cited 3800 times as of Jul. 2024)</i> <a href="https://gist.github.com/sergicastellasape/185f72fece3bb489f79366908594504d">[Source]</a>
                </p>
             </td>
         </tr>


         <tr>
          <td width="75%" valign="middle" style="position: relative; padding-left: 40px;">
            <br>
            <div style="position: absolute; top: 20px; left: 20px; height: 80%; border-left: 3px solid #000000;"><br></div>
                 <a href="https://arxiv.org/abs/2103.07976">
                     <papertitle> TransFG: A Transformer Architecture for Fine-grained Recognition </papertitle>
                 </a>
                 <br>
                 Ju He,
                 <strong>Jieneng Chen</strong>, Shuai Liu, Adam Kortylewski, Cheng Yang, Yutong Bai, Changhu Wang, Alan Yuille
                 <br>
                 <p></p>
                 In AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), 2022
                 <br>
                 <a href="https://arxiv.org/pdf/2103.07976.pdf">Paper</a> |
                 <a href="https://github.com/TACJu/TransFG">Code</a>
                    <br>
                    <br>
              <i style="color: red;">Top 3 Most Influential AAAI 2023 Papers</i> <a href="https://www.paperdigest.org/2023/04/most-influential-aaai-papers-2023-04/">[Source]</a>
             </td>
         </tr>

         <tr>
          <td width="75%" valign="middle" style="position: relative; padding-left: 40px;">
            <br>
            <div style="position: absolute; top: 20px; left: 20px; height: 77%; border-left: 3px solid #000000;"><br></div>
                 <a href="https://arxiv.org/abs/2105.05537">
                     <papertitle> Swin-Unet: Unet-like Pure Transformer for Medical Image Segmentation </papertitle>
                 </a>
                 <br>
                 Hu Cao, Yueyue Wang, <strong>Jieneng Chen</strong>, Dongsheng Jiang, Xiaopeng Zhang, Qi Tian, Manning Wang
                 <br>
                 <p></p>
                 In European Conference on Computer Vision (<strong>ECCV</strong>), 2022
                 <br>
                 <a href="https://arxiv.org/abs/2105.05537">Paper</a> |
                 <a href="https://github.com/HuCaoFighting/Swin-Unet">Code</a> | <img src="https://img.shields.io/github/stars/HuCaoFighting/Swin-Unet?style=social&label=Star&maxAge=2592000" alt="GitHub Stars Badge" style="margin-top: -3px; width:auto; height:22px;"> 
                    <br>
                    <br>
              <i style="color: red;">Top 3 Most Cited ECCV Papers in Five Years According to Google Metrics</i> <a href="https://scholar.google.com/citations?hl=en&vq=en&view_op=list_hcore&venue=cwIh2C-xo8kJ.2024">[Source]</a>
            <br>
            <br>
             </td>
         </tr>


         <tr>
          <td width="75%" valign="middle" style="position: relative; padding-left: 40px;">
            <br>
            <div style="position: absolute; top: 20px; left: 20px; height: 78%; border-left: 3px solid #000000;"><br></div>
                 <a href="https://ojs.aaai.org/index.php/AAAI/article/view/17066">
                     <papertitle> Semi-supervised Medical Image Segmentation Through Dual-task Consistency</papertitle>
                 </a>
                 <br>
                 Xiangde Luo, <strong>Jieneng Chen</strong>, Tao Song, Guotai Wang
                 <br>
                 <p></p>
                 In AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), 2021
                 <br>
                 <a href="https://ojs.aaai.org/index.php/AAAI/article/view/17066">Paper</a> |
                 <a href="https://github.com/HiLab-git/DTC">Code</a>
                    <br>
                    <br>
              <i style="color: red;">Top 15 Most Influential AAAI 2021 Papers</i> <a href="https://www.paperdigest.org/2024/05/most-influential-aaai-papers-2024-05/">[Source]</a>
            <br>
            <br>
             </td>
         </tr>





         <tr>
          <td width="75%" valign="middle" style="position: relative; padding-left: 40px;">
            <br>
            <div style="position: absolute; top: 22px; left: 20px; height: 70%; border-left: 3px solid #000000;"></div>
                 <a href="https://arxiv.org/abs/2103.05170">
                     <papertitle> Sequential Learning on Liver Tumor Boundary Semantics and Prognostic Biomarker Mining </papertitle>
                 </a>
                 <br>
                 <strong>Jieneng Chen</strong>, Ke Yan, Yu-Dong Zhang, Youbao Tang, Xun Xu, Shuwen Sun, Qiuping Liu, Lingyun Huang, Jing Xiao, Alan L Yuille, Ya Zhang, Le Lu
                 <br>
                 <p></p>
                In International Conference on Medical Image Computing and Computer-Assisted Intervention (<strong>MICCAI</strong>), 2021
                 <br>
                 <a href="https://arxiv.org/pdf/2103.05170.pdf">Paper</a> | Early Accept | Travel Award (top 10%)
                </p>
                 <br>
             </td>
         </tr>


      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:10px;width:100%;vertical-align:middle">
            <heading>Service & Teaching</heading>
          </td>
        </tr>
      </tbody></table>
    <ul> 
      <li><strong>Serving</strong>: He is invited reviewers and program committees for major conference and journals, such as TPAMI, CVPR, ICCV, ECCV, NeurIPS, ICML, ICLR, TMI and MICCAI. He is on the workshop organizing committee for MICCAI and ISBI. </p>
      </li> 
    </ul>
      <ul> 
        <li><strong>Teaching Assistant</strong>: He is the head teaching assistant of <a href="https://www.cs.jhu.edu/~ayuille/JHUcourses/VisionAsBayesianInference2024Spring/601.783.html">Vision as Bayesian Inference</a> at JHU in Spring 2024.  He is the course assistant of <a href="https://deep.cs.jhu.edu/deeplearning.html"> Machine Learning: Deep Learning</a> at JHU in Spring 2022. </li>

        <li><strong>Lecturer</strong>: He is the guest lecturer for <a href="https://ei.jhu.edu/">Johns Hopkins Engineering Innovation Pre-College Programs</a>.</li> 
      </ul>
        </td>
        <br>
         </tr>




 <script xml:space="preserve" language="JavaScript">
 hideallbibs();
 </script>

 <!-- Google Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-131560165-1', 'auto');
  ga('send', 'pageview');
</script>
<!-- End Google Analytics -->

</body>

</html>
