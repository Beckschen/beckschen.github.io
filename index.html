<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'UA-7580334-2');
  </script>

  <title>Jieneng Chen</title>
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css">
  
  <meta name="author" content="Jieneng Chen">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="stylesheet" type="text/css" href="css/style.css">
  <link rel="icon" type="image/png" href="myimages/icon_jhu.jpeg">
  <style>
    pre {
        background-color: #f5f5f5;
        padding: 0px;
        border: 1px solid #ccc;
        border-radius: 5px;
        overflow: auto;
    }
    code {
        font-family: 'Courier New', Courier, monospace;
        font-size: 8px;
        color: #333;
    }
    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 16px
    }
</style>
</head>

  <table style="width:100%;max-width:1100px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:70%;vertical-align:middle">
              <p style="text-align:center">
                <name>Jieneng Chen</name>
              </p>
              <p>I'm a fifth-year Ph.D. candidate in <a href="https://www.cs.jhu.edu/">Computer Science</a> at <a href="https://www.jhu.edu/">Johns Hopkins University</a>, advised by Distinguished Professor <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>. 
              </p>

              <p>
                My research focuses on artificial intelligence, computer vision, multimodality, medical AI and embodied AI. 
                </p>  

            <span style="color:red">I am on the job market for 2025!</span> Would love to chat more if you are interested. I am also happy to give talks on my research in related seminars.

              
              <p style="text-align:center">
                </br>
                <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
                <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
                
                <a href="https://scholar.google.com/citations?hl=en&user=yLYj88sAAAAJ"><i class="ai ai-google-scholar ai-3x" style='font-size:30px'></i>&nbsp &nbsp
                <a href="mailto:jienengchen01@gmail.com"><i class="fa fa-envelope" style='font-size:29px'></i>&nbsp &nbsp
                <a href="https://github.com/Beckschen"><i class="fa fa-github" style='font-size:30px'></i>&nbsp &nbsp
                <a href="https://www.linkedin.com/in/jieneng-chen-53254011a/"><i class="fa fa-linkedin" style='font-size:30px'></i>&nbsp &nbsp
              
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <a href="myimages/photo.jpg"><img style="width:80%;max-width:80%;padding:0;border:1px solid #f2f3f3" alt="profile photo" src="myimages/photo.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <td style="padding:10px;width:100%;vertical-align:middle">
            <heading>Research Themes</heading>
            <br>
            My research is driven by a desire to understand the computational foundations of human-level and expert-level intelligence.  I strive to replicate human intelligence by developing general-purpose <strong>spatial and embodied intelligence</strong>, and to build expert-level intelligence to address challenges like health inequity and climate change.
          </td>
      </table>
          <p>
            <li style="padding-left:30px"><stronger>Spatial Intelligence</stronger>: 
              my vision is grounded in interpreting 3D spatial configurations and advancing spatial reasoning capabilities. 
              (1) develop vision models capable of predicting 3D orientation and geometry of objects from the monocular image and video. 
              (2) <a href="https://3dsrbench.github.io/">benchmark</a> and bridge the (significant) gap in 3D spatial reasoning capabilities between humans and GPT models.
            </li>
          </p>
          <p>
            <li style="padding-left:30px"><stronger>Embodied Intelligence</stronger>: 
              üß† I am developing human-like multi-sensory embodied agents and working towards addressing one of the most critical challengs in embodied AI --- planning with partial observations. 
              (1) leveraging video generative models for <a href="https://arxiv.org/pdf/2411.11844">imaginative world exploration</a>. 
              (2) enhance the reasoning capabilities of policy models (e.g., multimodal GPT) by improving visual representations in <a href="https://arxiv.org/pdf/2404.02132.pdf">visual encoders</a> and <a href="https://arxiv.org/abs/2406.20092">language models</a>.
              (3) equip embodied agents with <strong>spatial intelligence</strong> to navigate dynamic physical environments.
            </li>
          </p>
          <p>
            <li style="padding-left:30px"><stronger>Medical Intelligence</stronger>:
              my vision is grounded in foundational algorithms and scalable, clinically relevant applications to address health inequity üôè.
              <!-- (1) algorithms: I have developed two of the earliest Transformers for medical image analysis, specifically <a href="https://arxiv.org/pdf/2102.04306">TransUNet</a> and <a href="https://arxiv.org/abs/2105.05537">Swin-UNet</a> (8000 citations at total).  -->
              (1) algorithms: I developed the <a href="https://arxiv.org/pdf/2102.04306">TransUNet</a>, pioneering the Transformer era in medical image analysis.
              (2) clinical applications: I developed the <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_CancerUniT_Towards_a_Single_Unified_Model_for_Effective_Detection_Segmentation_ICCV_2023_paper.pdf">CancerUnit</a> to effectively detect and diagnose of eight major cancers using <strong>3D spatial analysis</strong>. 
              Once detected, I monitored the cancer's prognosis using multi-scale <strong>spatial</strong> analysis in <a href="https://link.springer.com/chapter/10.1007/978-3-030-87234-2_72">CT</a> and <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC8264317/">IHC</a>. 
            </li>
          </p>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <td style="padding:10px;width:100%;vertical-align:middle">
              <heading>News</heading>
            </td>
        </table>
          <li style="padding-left:30px"> I will be offering a brand new course 'Machine Imagination' for students of all levels at JHU in Winter 2025.
            <li style="padding-left:30px"> selected as a <a href="https://www.businesswire.com/news/home/20240920559594/en/Siebel-Scholars-Foundation-Announces-Class-of-2025"><strong>Siebel Scholar</strong></a>, recognizing exceptional students from the world‚Äôs leading graduate schools.
            <li style="padding-left:30px">  <a href="https://arxiv.org/pdf/2404.02132.pdf">my algorithms</a> have been hosted in the world-renowned <a href="https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/vitamin.py">timm</a> <img src="https://img.shields.io/github/stars/huggingface/pytorch-image-models?style=social&label=Star&maxAge=2592000" alt="GitHub Stars Badge" style="margin-top: -3px; width:auto; height:18px;"> and <a href="https://github.com/mlfoundations/open_clip/blob/main/src/open_clip/model_configs/ViTamin-XL-384.json">open_clip</a> <img src="https://img.shields.io/github/stars/mlfoundations/open_clip?style=social&label=Star&maxAge=2592000" alt="GitHub Stars Badge" style="margin-top: -3px; width:auto; height:18px;">.
            <li style="padding-left:30px"> our paper TransUNet is listed as one of <a href="https://gist.github.com/sergicastellasape/185f72fece3bb489f79366908594504d">the top 15 cited 2021 paper in all AI fields</a>. 
            <li style="padding-left:30px"> our paper TransFG is listed as one of  <a href="https://www.paperdigest.org/2023/04/most-influential-aaai-papers-2023-04/">top 3 most influential AAAI 2023 papers</a>.
            <li style="padding-left:30px">our paper  SwinUNet is listed as one of   <a href="https://scholar.google.com/citations?hl=en&vq=en&view_op=list_hcore&venue=cwIh2C-xo8kJ.2024">top 3 most cited ECCV papers in five years in Google Metrics</a>.

            </li>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <heading>Recent Publications (last year)</heading>
            </td>
          </tr>
        </tbody>
      </table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>        


          <p><font size="4" style="position: relative; padding-left: 20px;"><a href="https://scholar.google.com/citations?hl=en&user=yLYj88sAAAAJ">Full list on Google Scholar Profile</a> <smaller>(10+ first-authored, an h-idex of 22, with 10,500 citations).</smaller> </font> 


          </p> 


          <tr>
            <td width="75%" valign="middle" style="position: relative; padding-left: 40px;">
              <div style="position: absolute; top: 18px; left: 20px; height: 70%; border-left: 3px solid #000000;"></div>
                   <br>
                  <p>
                  <a href="https://arxiv.org/pdf/2411.11844">
                      <papertitle>Generative World Explorer</papertitle>
                  </a>
                  <br>
                  <a href="https://openreview.net/profile?id=~TaiMing_Lu1", class="author">Taiming Lu</a>, 
                  <a href="https://www.tshu.io/", class="author">Tianmin Shu</a>,
                  <a href="https://www.cs.jhu.edu/~ayuille/", class="author">Alan Yuille</a>,
                  <a href="https://danielkhashabi.com/", class="author">Daniel Khashabi</a>,
                  <strong>Jieneng Chen</strong>
                  <br>
                  <p></p>
                  Technical Report, ü§ó #1 <a href="https://huggingface.co/papers/2411.11844">Hugging Face Daily Papers</a>, 2024
                  <br>
                  <a href="https://arxiv.org/pdf/2411.11844">Paper</a> |
                      <a href="https://github.com/Beckschen/genex">Code</a> |
                        <a href="https://generative-world-explorer.github.io/">Project</a> | <a href="https://youtu.be/_1YMpI-oHWU">Video</a> 
                       
                </p>
                 <br>
             </td>
         </tr>

         <tr>
          <td width="75%" valign="middle" style="position: relative; padding-left: 40px;">
            <div style="position: absolute; top: 18px; left: 20px; height: 70%; border-left: 3px solid #000000;"></div>
                 <br>
                <p>
                <a href="https://3dsrbench.github.io/">
                    <papertitle>3DSRBench: A Comprehensive 3D Spatial Reasoning Benchmark</papertitle>
                </a>
                <br>
                <a href="https://wufeim.github.io/", class="author">Wufei Ma</a>, 
                <a href="https://www.linkedin.com/in/haoyuchen050400/?locale=en_US", class="author">Haoyu Chen</a>, 
                <a href="https://openreview.net/profile?id=~Guofeng_Zhang4", class="author">Guofeng Zhang</a>, 
                <a href="https://celsodemelo.net/", class="author">Celso Miguel de Melo</a>,
                <a href="https://www.cs.jhu.edu/~ayuille/", class="author">Alan Yuille</a>,
                <strong>Jieneng Chen</strong>
                <br>
                <p></p>
                Technical Report, 2024
                <br>
                <a href="https://3dsrbench.github.io/">Paper</a> |
                    <a href="https://3dsrbench.github.io/">Data</a> |
                      <a href="https://3dsrbench.github.io/">Project</a>  
                     
              </p>
               <br>
           </td>
       </tr>
         

          <tr>
            <td width="75%" valign="middle" style="position: relative; padding-left: 40px;">
              <div style="position: absolute; top: 18px; left: 20px; height: 70%; border-left: 3px solid #000000;"></div>
                   <br>
                  <p>
                  <a href="https://arxiv.org/abs/2406.20092">
                      <papertitle>Efficient Large Multi-modal Models via Visual Context Compression</papertitle>
                  </a>
                  <br>
                  <strong>Jieneng Chen</strong>, 
                  <a href="https://openreview.net/profile?id=~Luoxin_Ye1", class="author">Luoxin Ye</a>,
                  <a href="https://tacju.github.io/", class="author">Ju He</a>,
                  <a href="https://aiem.jhu.edu/people/zhaoyang-wang/", class="author">Zhaoyang Wang</a>,
                  <a href="https://danielkhashabi.com/", class="author">Daniel Khashabi</a>,
                  <a href="https://www.cs.jhu.edu/~ayuille/", class="author">Alan Yuille</a>
                  <br>
                  <p></p>
                  In Neural Information Processing Systems (<strong>NeurIPS</strong>), 2024
                  <br>
                  <a href="https://arxiv.org/pdf/2406.20092">Paper</a> |
                      <a href="https://github.com/Beckschen/LLaVolta">Code</a> |
                        <a href="https://beckschen.github.io/llavolta.html">Project</a>
                </p>
                 <br>
             </td>
         </tr>

          <tr>
            <td width="75%" valign="middle" style="position: relative; padding-left: 40px;">
              <div style="position: absolute; top: 5px; left: 20px; height: 75%; border-left: 3px solid #000000;"></div>
                  <p>
                  <a href="https://arxiv.org/pdf/2404.02132.pdf">
                      <papertitle>Designing Scalable Vision Models in the Vision-Language Era</papertitle>
                  </a>
                  <br>
                  <strong>Jieneng Chen</strong>, 
                  <a href="https://www.cs.jhu.edu/~ayuille/", class="author"> Qihang Yu</a>, 
                  <a href="https://www.cs.jhu.edu/~ayuille/", class="author">Xiaohui Shen</a>, 
                  <a href="https://www.cs.jhu.edu/~ayuille/", class="author">Alan Yuille</a>, 
                  <a href="http://liangchiehchen.com/", class="author">Liang-Chieh Chen</a>
                  <br>
                  <p></p>
                  In Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2024
                  <br>
                  <a href="https://arxiv.org/pdf/2404.02132.pdf">Paper</a> |
                      <a href="https://github.com/Beckschen/ViTamin">Code</a> |
                        <a href="https://huggingface.co/jienengchen/ViTamin-XL-384px">ü§ó HuggingFace</a> | <a href="https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/vitamin.py">timm</a> <img src="https://img.shields.io/github/stars/huggingface/pytorch-image-models?style=social&label=Star&maxAge=2592000" alt="GitHub Stars Badge" style="margin-top: -3px; width:auto; height:16px;"> | <a href="https://github.com/mlfoundations/open_clip/blob/main/src/open_clip/model_configs/ViTamin-XL-384.json">open_clip</a> <img src="https://img.shields.io/github/stars/mlfoundations/open_clip?style=social&label=Star&maxAge=2592000" alt="GitHub Stars Badge" style="margin-top: -3px; width:auto; height:16px;">
                  
                  </p>
                <br>
             </td>
         </tr>


          <tr>
            <td width="75%" valign="middle" style="position: relative; padding-left: 40px;">
              <div style="position: absolute; top: 18px; left: 20px; height: 70%; border-left: 3px solid #000000;"></div>
                   <br>
                  <p>
                  <a href="https://arxiv.org">
                      <papertitle>3D-Gait: Virtual Marker-Driven 3D Representation for Multi-Modal Gait Recognition</papertitle>
                  </a>
                  <br> 
                  <a href="https://aiem.jhu.edu/people/zhaoyang-wang/", class="author">Zhaoyang Wang</a>,
                  <a href="https://joellliu.github.io/", class="author">Jiang Liu</a>,
                  <strong>Jieneng Chen</strong>, 
                  <a href="https://engineering.jhu.edu/faculty/rama-chellappa/", class="author">Rama Chellappa</a>
                  <br>
                  <p></p>
                  to be appeared in Winter Conference on Applications of Computer Vision (<strong>WACV</strong>), 2025
                  <br>
                </p>
                 <br>
             </td>
         </tr>

         <tr>
          <td width="75%" valign="middle" style="position: relative; padding-left: 40px;"> 
            <div style="position: absolute; top: 5px; left: 20px; height: 82%; border-left: 3px solid #000000;"></div>
                 <p>
                     <a href="https://www.sciencedirect.com/science/article/pii/S1361841524002056">
                         <papertitle>TransUNet: Rethinking the U-Net Architecture Design for Medical Image Segmentation through the Lens of Transformers</papertitle>
                     </a>
                     <br>
                     <strong>Jieneng Chen</strong>, 
                     <a href="https://www.sciencedirect.com/science/article/pii/S1361841524002056", class="author">Jieru Mei</a>, 
                    <a href="https://www.sciencedirect.com/science/article/pii/S1361841524002056", class="author">Xianhang Li</a>, 
                    <a href="https://www.sciencedirect.com/science/article/pii/S1361841524002056", class="author">Yongyi Lu</a>, 
                    <a href="https://www.sciencedirect.com/science/article/pii/S1361841524002056", class="author">Qihang Yu</a>, 
                    <a href="https://www.sciencedirect.com/science/article/pii/S1361841524002056", class="author">Qingyue Wei</a>, 
                    <a href="https://www.sciencedirect.com/science/article/pii/S1361841524002056", class="author">Xiangde Luo</a>, 
                    <a href="https://www.sciencedirect.com/science/article/pii/S1361841524002056", class="author">Yutong Xie</a>, 
                    <a href="https://www.sciencedirect.com/science/article/pii/S1361841524002056", class="author">Ehsan Adeli</a>, 
                    <a href="https://www.sciencedirect.com/science/article/pii/S1361841524002056", class="author">Yan Wang</a>, 
                    <a href="https://www.sciencedirect.com/science/article/pii/S1361841524002056", class="author">Matthew P Lungren</a>, 
                    <a href="https://www.sciencedirect.com/science/article/pii/S1361841524002056", class="author">Shaoting Zhang</a>, 
                    <a href="https://www.sciencedirect.com/science/article/pii/S1361841524002056", class="author">Lei Xing</a>, 
                    <a href="https://www.sciencedirect.com/science/article/pii/S1361841524002056", class="author">Le Lu</a>, 
                    <a href="https://www.sciencedirect.com/science/article/pii/S1361841524002056", class="author">Alan Yuille</a>, 
                    <a href="https://www.sciencedirect.com/science/article/pii/S1361841524002056", class="author">Yuyin Zhou</a>
                     <br>
                     <p></p>
                     Medical Image Analysis (MedIA), 2024
                     <p></p>
                      <a href="https://arxiv.org/pdf/2102.04306.pdf">ICML-W 2021</a> | 
                      <a href="https://www.sciencedirect.com/science/article/pii/S1361841524002056">Journal</a> |
                      <a href="https://github.com/Beckschen/TransUNet">Code</a> |
                      <img src="https://img.shields.io/github/stars/Beckschen/TransUNet?style=social&label=Star&maxAge=2592000" alt="GitHub Stars Badge" style="margin-top: -3px; width:auto; height:22px;"> 
                      <br>
                      <i style="color: red;">Top ScienceDirect downloaded articles in 2024, published all time</i>
                      <br>
                      <i style="color: red;">Top 15 cited 2021 paper in all AI fields (cited 4K times as of 2024) for arXiv version</i> <a href="https://gist.github.com/sergicastellasape/185f72fece3bb489f79366908594504d">[Source]</a>
                      
                </p>
                <br>
             </td>
         </tr>


          <!-- <tr>
            <td width="75%" valign="middle" style="position: relative; padding-left: 40px;">
              <div style="position: absolute; top: 5px; left: 20px; height: 75%; border-left: 3px solid #000000;"></div>
                  <p>
                  <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_CancerUniT_Towards_a_Single_Unified_Model_for_Effective_Detection_Segmentation_ICCV_2023_paper.pdf">
                      <papertitle>Towards a Single Unified Model for Effective Detection, Segmentation, and Diagnosis of Eight Major Cancers</papertitle>
                  </a>
                  <br>
                  <strong>Jieneng Chen</strong>, Yingda Xia, Jiawen Yao, Ke Yan, Jianpeng Zhang, Le Lu, .., Jingren Zhou, Alan Yuille, Zaiyi Liu, Ling Zhang
                  <br>
                  <p></p>
                  In International Conference on Computer Vision (<strong>ICCV</strong>), 2023
                  <br>
                      <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_CancerUniT_Towards_a_Single_Unified_Model_for_Effective_Detection_Segmentation_ICCV_2023_paper.pdf">Paper</a> 
                </p>
                 <br>
             </td>
         </tr>

        <tr>
            <td width="75%" valign="middle" style="position: relative; padding-left: 40px;">
              <div style="position: absolute; top: 5px; left: 20px; height: 70%; border-left: 3px solid #000000;"></div>
                  <p>
                  <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/He_Compositor_Bottom-Up_Clustering_and_Compositing_for_Robust_Part_and_Object_CVPR_2023_paper.pdf">
                      <papertitle>Compositor: Bottom-up Clustering and Compositing for Robust Part and Object Segmentation</papertitle>
                  </a>
                  <br>
                  Ju He *, <strong>Jieneng Chen *</strong>, Mingxian Lin, Qihang Yu, Alan Yuille
                  <br>
                  <p></p>
                  In Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2023
                  <br>
                      <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/He_Compositor_Bottom-Up_Clustering_and_Compositing_for_Robust_Part_and_Object_CVPR_2023_paper.pdf">Paper</a> |
                      * Equal contributed
                </p>
                 <br>
             </td>
         </tr>

        <tr>
            <td width="75%" valign="middle" style="position: relative; padding-left: 40px;">
              <div style="position: absolute; top: 5px; left: 20px; height: 70%; border-left: 3px solid #000000;"></div>
                  <p>
                  <a href="https://arxiv.org/abs/2111.09833?context=cs">
                      <papertitle>TransMix: Attend to Mix for Vision Transformers</papertitle>
                  </a>
                  <br>
                  <strong>Jieneng Chen</strong>, Shuyang Sun, Ju He, Philip Torr, Alan Yuille, Song Bai
                  <br>
                  <p></p>
                  In Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2022
                  <br>
                      <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_TransMix_Attend_To_Mix_for_Vision_Transformers_CVPR_2022_paper.pdf">Paper</a> | 
                      <a href="https://github.com/Beckschen/TransMix">Code</a> 
                </p>
                 <br>
             </td>
         </tr> -->



         <!-- <tr>
          <td width="75%" valign="middle" style="position: relative; padding-left: 40px;"> 
            <div style="position: absolute; top: 5px; left: 20px; height: 92%; border-left: 3px solid #000000;"></div>
                 <p>
                     <a href="https://arxiv.org/pdf/2102.04306.pdf">
                         <papertitle>TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation</papertitle>
                     </a>
                     <br>
                     <strong>Jieneng Chen</strong>, Yongyi Lu, Qihang Yu, Xiangde Luo, Ehsan Adeli, Yan Wang, Le Lu, Alan Yuille, Yuyin Zhou
                     <br>
                     <p></p>
                     International Conference on Marchine Learning (<strong>ICML</strong>) workshop, arXiv 2021
                     <p></p>
                     <a href="https://arxiv.org/pdf/2310.07781">3D-TransUNet</a> is published in Medical Image Analysis, 2024 (IF>10)
                     <br>
                      <a href="https://arxiv.org/pdf/2102.04306.pdf">Paper</a> |
                      <a href="https://github.com/Beckschen/TransUNet">Code</a> |
                      <img src="https://img.shields.io/github/stars/Beckschen/TransUNet?style=social&label=Star&maxAge=2592000" alt="GitHub Stars Badge" style="margin-top: -3px; width:auto; height:22px;"> 
                      <br>
                      <br>
              <i style="color: red;">Top 15 Cited 2021 Paper in All AI Fields (Cited 4K times as of 2024)</i> <a href="https://gist.github.com/sergicastellasape/185f72fece3bb489f79366908594504d">[Source]</a>
                </p>
             </td>
         </tr>


         <tr>
          <td width="75%" valign="middle" style="position: relative; padding-left: 40px;">
            <br>
            <div style="position: absolute; top: 20px; left: 20px; height: 80%; border-left: 3px solid #000000;"><br></div>
                 <a href="https://arxiv.org/abs/2103.07976">
                     <papertitle> TransFG: A Transformer Architecture for Fine-grained Recognition </papertitle>
                 </a>
                 <br>
                 Ju He,
                 <strong>Jieneng Chen</strong>, Shuai Liu, Adam Kortylewski, Cheng Yang, Yutong Bai, Changhu Wang, Alan Yuille
                 <br>
                 <p></p>
                 In AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), 2022
                 <br>
                 <a href="https://arxiv.org/pdf/2103.07976.pdf">Paper</a> |
                 <a href="https://github.com/TACJu/TransFG">Code</a>
                    <br>
                    <br>
              <i style="color: red;">Top 3 Most Influential AAAI 2023 Papers</i> <a href="https://www.paperdigest.org/2023/04/most-influential-aaai-papers-2023-04/">[Source]</a>
             </td>
         </tr>

         <tr>
          <td width="75%" valign="middle" style="position: relative; padding-left: 40px;">
            <br>
            <div style="position: absolute; top: 20px; left: 20px; height: 77%; border-left: 3px solid #000000;"><br></div>
                 <a href="https://arxiv.org/abs/2105.05537">
                     <papertitle> Swin-Unet: Unet-like Pure Transformer for Medical Image Segmentation </papertitle>
                 </a>
                 <br>
                 Hu Cao, Yueyue Wang, <strong>Jieneng Chen</strong>, Dongsheng Jiang, Xiaopeng Zhang, Qi Tian, Manning Wang
                 <br>
                 <p></p>
                 In European Conference on Computer Vision (<strong>ECCV</strong>), 2022
                 <br>
                 <a href="https://arxiv.org/abs/2105.05537">Paper</a> |
                 <a href="https://github.com/HuCaoFighting/Swin-Unet">Code</a> | <img src="https://img.shields.io/github/stars/HuCaoFighting/Swin-Unet?style=social&label=Star&maxAge=2592000" alt="GitHub Stars Badge" style="margin-top: -3px; width:auto; height:22px;"> 
                    <br>
                    <br>
              <i style="color: red;">Top 3 Most Cited ECCV Papers in Five Years According to Google Metrics</i> <a href="https://scholar.google.com/citations?hl=en&vq=en&view_op=list_hcore&venue=cwIh2C-xo8kJ.2024">[Source]</a>
            <br>
            <br>
             </td>
         </tr>


         <tr>
          <td width="75%" valign="middle" style="position: relative; padding-left: 40px;">
            <br>
            <div style="position: absolute; top: 20px; left: 20px; height: 78%; border-left: 3px solid #000000;"><br></div>
                 <a href="https://ojs.aaai.org/index.php/AAAI/article/view/17066">
                     <papertitle> Semi-supervised Medical Image Segmentation Through Dual-task Consistency</papertitle>
                 </a>
                 <br>
                 Xiangde Luo, <strong>Jieneng Chen</strong>, Tao Song, Guotai Wang
                 <br>
                 <p></p>
                 In AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), 2021
                 <br>
                 <a href="https://ojs.aaai.org/index.php/AAAI/article/view/17066">Paper</a> |
                 <a href="https://github.com/HiLab-git/DTC">Code</a>
                    <br>
                    <br>
              <i style="color: red;">Top 15 Most Influential AAAI 2021 Papers</i> <a href="https://www.paperdigest.org/2024/05/most-influential-aaai-papers-2024-05/">[Source]</a>
            <br>
            <br>
             </td>
         </tr>





         <tr>
          <td width="75%" valign="middle" style="position: relative; padding-left: 40px;">
            <br>
            <div style="position: absolute; top: 22px; left: 20px; height: 70%; border-left: 3px solid #000000;"></div>
                 <a href="https://arxiv.org/abs/2103.05170">
                     <papertitle> Sequential Learning on Liver Tumor Boundary Semantics and Prognostic Biomarker Mining </papertitle>
                 </a>
                 <br>
                 <strong>Jieneng Chen</strong>, Ke Yan, Yu-Dong Zhang, Youbao Tang, Xun Xu, Shuwen Sun, Qiuping Liu, Lingyun Huang, Jing Xiao, Alan L Yuille, Ya Zhang, Le Lu
                 <br>
                 <p></p>
                In International Conference on Medical Image Computing and Computer-Assisted Intervention (<strong>MICCAI</strong>), 2021
                 <br>
                 <a href="https://arxiv.org/pdf/2103.05170.pdf">Paper</a> | Early Accept | Travel Award (top 10%)
                </p>
                 <br>
             </td>
         </tr> -->


      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:10px;width:100%;vertical-align:middle">
            <heading>Teaching & Service</heading>
          </td>
        </tr>
      </tbody></table>

      <ul> 
        <li><strong>Instructor</strong>: He is the instructor for a course in Winter 2025 at JHU.</li> 
        <li><strong>Teaching assistant</strong>: 
          <ul>
            <li>head teaching assistant, <a href="https://www.cs.jhu.edu/~ayuille/JHUcourses/VisionAsBayesianInference2024Spring/601.783.html">Vision as Bayesian Inference</a> at JHU in Spring 2024.</li>
            <li>course assistant, <a href="https://deep.cs.jhu.edu/deeplearning.html"> Machine Learning: Deep Learning</a> at JHU in Spring 2022. </li>
          </ul>
        </li>
        <li><strong>Guest lecturer</strong>
          <ul> 
            <li><a href="https://ei.jhu.edu/">Engineering Innovation Pre-College Programs</a>, at JHU, 2024</li>
            <li>CSE290D: Neural Computation, at UC Santa Cruz, 2024. </li>
          </ul>
        </li> 
      </ul>
    <ul> 
      <li><strong>Serving</strong>: He is on the invited reviewers and program committees for major conference and journals, such as CVPR, ICCV, ECCV, NeurIPS, ICML, ICLR, AAAI, TPAMI, TMI and MICCAI. </p> He provided mentor hours for PhD students and underrepresented students at JHU.
      </li> 
    </ul>
        </td>
        <br>
         </tr>




 <script xml:space="preserve" language="JavaScript">
 hideallbibs();
 </script>

 <!-- Google Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-131560165-1', 'auto');
  ga('send', 'pageview');
</script>
<!-- End Google Analytics -->

</body>

</html>
